{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cbcbe07-38c8-40e8-ae03-0527ddbf3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "# \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "# \"the-verdict.txt\")\n",
    "# file_path = \"the-verdict.txt\"\n",
    "# urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f203b22f-d3b3-4be5-a0b9-c177ae875e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    print(\"Total number of character:\", len(raw_text))\n",
    "    print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a1e1b6-053c-46c1-86fb-959fe0b1a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fb907d-82b2-445a-b4f2-2b78bcdd885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f943c4-3834-4136-a4e8-762a12fb9984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d734e2-26cd-4f68-b4bf-8f073ae8ecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb549a58-0fb9-479c-a11f-2e7605ccffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81f7617-a537-496e-af80-319284759c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e74a210-0116-46db-bfc7-ec45cc5ea3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa68121-9bbe-4105-959f-ad1814e3a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindles', 'HAD', 'Had', 'Hang', 'Has', 'He', 'Her', 'Hermia', 'His', 'How', 'I', 'If', 'In', 'It', 'Jack', 'Jove', 'Just', 'Lord', 'Made', 'Miss', 'Money', 'Monte', 'Moon-dancers', 'Mr', 'Mrs', 'My', 'Never', 'No', 'Now', 'Nutley', 'Of', 'Oh', 'On', 'Once', 'Only', 'Or', 'Perhaps', 'Poor', 'Professional', 'Renaissance', 'Rickham', 'Riviera', 'Rome', 'Russian', 'Sevres', 'She', 'Stroud', 'Strouds', 'Suddenly', 'That', 'The', 'Then', 'There', 'They', 'This', 'Those', 'Though']\n"
     ]
    }
   ],
   "source": [
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7149aef-b69b-4b7c-bf13-db6bcac582a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ee371c-36ef-4ef0-b638-84f387686d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer class \n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a908e3-b569-4c67-853c-e0a798b1aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a41e05-3f0f-4051-9306-875958e56978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee162a9-8012-4a2c-8d38-bfd850600481",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "# print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d1e398a-97a6-495b-871e-8ce670db6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c64bbca-62f7-44a1-a414-02cd360db0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9df98237-c066-42a2-81f1-26ba4f9efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "        else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0138f130-d443-4bf7-a25a-25f74b4b520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a9f237-ceae-48e5-bec7-3afb2328e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816d1d09-0c81-471e-9f2d-8dfeadca6f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a859fbba-3d8c-4831-be21-6188dfedead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fa31400-d075-436d-87e7-42402b0ab7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tiktoken.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d096a04-3eea-4102-b7db-7572342b4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cf948ba-1ef1-4d7b-aae2-a25ab2ba12cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39e0d0c6-93aa-4501-b33d-c58f31e0786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e3d9d86-f2d8-4023-af68-9d60375650ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad3d1331-b55d-4007-8914-064a01f08e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd2d97cb-fa5c-4fc1-b56b-2f7652cbcfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]\n"
     ]
    }
   ],
   "source": [
    "print(enc_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e75de4c6-e8bd-4dc1-891c-59cba800f134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow,'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(enc_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "896c7b3d-4508-4512-9af2-5cdbaad613e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f5ca120-df7b-4b3d-8007-794208f8cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id in range(1, context_size+1):\n",
    "#     x = enc_sample[:id]\n",
    "#     y = enc_sample[id]\n",
    "#     print(f\"x = {x} y = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85e83c3c-7d16-4945-ac42-28e5657194ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e11b730-73be-49f3-8724-1c06417a242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "add579af-f2e7-4f82-81fa-94f2114afc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d6de6a0-1ef7-4c03-bf8c-fa1fd9e904b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "    stride=128, shuffle=True, drop_last=True,\n",
    "    num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    drop_last=drop_last,\n",
    "    num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a02c23a5-8036-43ee-b878-dae88152116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b851925b-4890-48e7-8309-97485ec807fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "print(next(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58c6df7c-e8d9-4343-9a1b-20d7848b7bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=8, max_length=4, stride=4,\n",
    "shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a35f55a-e804-4738-9c31-8fb97afebd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d09ddd4-f74d-4d27-8155-f8026c5062dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2882484-6bac-42c6-9b01-ec387523aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be6740dc-0159-4148-8f13-11e19a2d321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f07dac5a-b354-4d3f-8404-432eb30c5b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=8, max_length=max_length,\n",
    "stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe9986a9-8a38-45a9-8f6a-633ec341ee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b988d34-f66f-4853-a0e7-20e2ab8ac450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1780b36-4e29-4d51-a7a5-8747ca7c58bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4913,  1.1239,  1.4588,  ..., -0.3995, -1.8735, -0.1445],\n",
      "        [ 0.4481,  0.2536, -0.2655,  ...,  0.4997, -1.1991, -1.1844],\n",
      "        [-0.2507, -0.0546,  0.6687,  ...,  0.9618,  2.3737, -0.0528],\n",
      "        [ 0.9457,  0.8657,  1.6191,  ..., -0.4544, -0.7460,  0.3483]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3683671-c7ab-4c80-8c39-cc789b6bedb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2097e03f-1806-4277-b984-946ae81660d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your\n",
    "[0.55, 0.87, 0.66], # journey\n",
    "[0.57, 0.85, 0.64], # starts\n",
    "[0.22, 0.58, 0.33], # with\n",
    "[0.77, 0.25, 0.10], # one\n",
    "[0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8d647db-2cf5-4d21-b658-b05a9675d8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91ef3dce-659f-4ca7-b495-0e1c3320f8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f07dd33-48b8-4186-a988-781e6b57d7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2cf476b-054b-40b0-8431-8e51db0637f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57417b34-1bd5-46f9-a7a7-5e6bb3fb2d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5481b266-77d0-4565-8ab5-6fc66353ed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[2]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5068c0cc-4a32-4ec2-bb5f-340610408e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d71a30e-7d0d-4379-9373-5785e7a588b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a93589e2-5ed5-43d6-bbb2-63247ef10ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa8c8fe6-1c82-4c41-9773-9c7a478dba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "beb68526-478b-4504-adbc-1aa2bda57b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "025c828a-44cb-40e1-924b-28d599229d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05f18706-2c66-4046-884e-218825c328be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b2e63e8e-ada0-4f22-88fa-03750d46ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key= torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ea8115f-96f2-48ee-95d5-485dcfff4d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "679644c4-f313-4e9a-8b84-028e96c5ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a73b478e-b140-446f-841b-5fd9540c037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d447a24-611e-4b99-997f-fbe3f07a9c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d5feab5-83d0-4b8b-babb-7bd454fbbcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "104a2818-d391-4abf-8e1c-6f0201276c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d3a7680-a613-4018-86cb-bd43eda54a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b2cd115-6ba2-4e34-aa5c-62dd16825bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention snippet \n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c03936f-9a7d-4a17-a670-4c8b398f0bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9ece663-b541-4f0d-8cb6-f96063ff8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd modified self attention class \n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d361513b-e83e-4df2-b535-5c51c9314950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "527ca72a-9e16-4484-b058-968d535c2454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5b4a2e3-606d-40c9-b7dd-f096620ee02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "099857c6-8627-4b61-883c-07ff9884bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d92f0f6b-a18e-48b6-8d40-f8e49273b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cda38513-fe9c-4afc-8fd8-ac3974a725ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7eb20448-d663-4aa2-8126-112064c5957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58538a84-6ce1-4715-85ce-41cee81ad7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f69362f2-e3bb-4581-b46c-fbc8f913899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94fb4b79-6562-475b-9d5e-0ccf7c92197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9036494-ae97-47b1-8762-a3fc8ad1d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention class with causual attention code\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "    dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fea2b5e6-d73a-42ca-9ed2-5802896d741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75ad3835-31d0-47ca-ab99-0868a9f11e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "        dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "        [CausalAttention(\n",
    "        d_in, d_out, context_length, dropout, qkv_bias\n",
    "        )\n",
    "        for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "41f0b6fa-93ea-44da-829a-d8b2252feffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81dd5ee6-6ac5-41ee-bbac-1b03ccb0f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(\n",
    "d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bff06a7e-1b60-41f0-b3f1-fe06b7fd1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "        context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        \"mask\",\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "        b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ef4ec1b-7828-4010-bb2c-03a78e496973",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "                   \n",
    "                    [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8022449-b6b4-4583-92df-d977499ba3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b1be3a80-fe6c-44f6-89a6-96ad89ae5bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c04be45c-5a01-4433-89df-b337d27356b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d65a87df-b768-4ec0-b2c6-23f39b2d0db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "17457cec-5a8a-478a-a964-adf54f498272",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,\n",
    "\"context_length\": 256,\n",
    "\"emb_dim\": 768,\n",
    "\"n_heads\": 12,\n",
    "\"n_layers\": 12,\n",
    "\"drop_rate\": 0.1,\n",
    "\"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "deac1804-d23d-4221-8f74-15332ca8baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 code \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "                    *[DummyTransformerBlock(cfg)\n",
    "                    for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a1f6b805-9a2a-42da-a82a-0753f8b86037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dff06e4a-181e-4fc2-9581-17a99c91c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0448,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5347bba7-608d-49bd-ad79-4d98df02f59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73f32418-be45-4f9b-bbac-969a7f562420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape : torch.Size([2, 5])\n",
      "batch data : tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"
     ]
    }
   ],
   "source": [
    "print('batch shape :', batch_example.shape)\n",
    "print('batch data :', batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8be0f394-e412-42e5-8f7e-2a4fc57cb083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=6, bias=True)\n",
       "  (1): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fde7dab1-8bed-4f4f-bf10-bd6cb7dab4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1324, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(out[0]) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "800b999e-ea1a-463b-b054-15505952b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8b5fef2a-aa4d-4c40-b1c7-cdf9a890b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0020a632-a4da-499a-a29e-7a5319f1e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9aa07fd5-902b-4ec6-a687-bafd577169fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "81c29b51-52c5-41d7-baaf-128b7c3bbcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "23922f11-649d-4788-b4ce-db6ac578705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "        (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c98390fe-2c16-4b9f-b7e4-f95b8ebb6df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7ded1b6-33ac-43b6-83f2-9d210afb9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5fed5705-79a1-4891-bf14-de57069fa30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 6, 0],\n",
       "        [5, 0, 5]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(10, (2,3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8e899d21-958c-413d-b8a5-d80d12966a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 10])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6d6282c7-977b-4bce-8bea-098f2f6cba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "604601aa-878a-4a33-a7f5-4a243990828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119d2aa-2a8b-46e1-b665-59c4d3fe8098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "660e4e25-9120-42de-89bb-f770cd01627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c669d25b-401e-4623-9e82-1fd881f12a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "                    nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "                        GELU()),\n",
    "                        nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "                        GELU()),\n",
    "                        nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "                        GELU()),\n",
    "                        nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "                        GELU()),\n",
    "                        nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "                        GELU())\n",
    "                        ])        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37631503-0caa-4e93-9085-0db548908e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "                            layer_sizes, use_shortcut=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e5912ce3-9c1e-40f8-ae31-a15eab390e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(param)\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0ab77255-0b36-4e05-a334-88770f84a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f73d0edf-67a2-4846-990a-382e507b3d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694108307361603\n",
      "layers.2.0.weight has gradient mean of 0.3289699852466583\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ffdffb49-77e3-4d70-aaf4-431673b2eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout=cfg[\"drop_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "342ae07c-4862-4a6c-ac92-88457f132c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc8c81ef-5b4a-4c87-bc65-53d3ab2d2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 model\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "22ced93e-7cac-41a5-8169-3e8be903451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f773c309-373d-4f0f-8c2b-8dff6b87e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "77f192b1-8fb1-4b17-bbbe-1d35941fa65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7be665926a40>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e9de5dc7-6e68-42c9-9757-6d562ce1bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d665de43-6ec7-4456-a04c-820c29ef03f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "total_params - sum(p.numel()\n",
    "for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b282846d-c721-4303-ab4f-48417b5e0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "54f31366-d603-4139-9b32-dc198cd90c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        print(idx_cond)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0c05a2d6-0a3e-436a-865e-05e8dec391ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "tensor([[15496,    11,   314,   716]])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "print(encoded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5b29368d-e050-4d78-81d4-c28efde1deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,    11,   314,   716]])\n",
      "tensor([[15496,    11,   314,   716, 27018]])\n",
      "tensor([[15496,    11,   314,   716, 27018, 24086]])\n",
      "tensor([[15496,    11,   314,   716, 27018, 24086, 47843]])\n",
      "tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961]])\n",
      "tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348]])\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "077b689b-2ff2-4ef5-a4b0-0375399a8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4d9e07cd-d715-4925-850f-141e6744ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "666b18a7-a127-4fb7-a7a0-946ab512a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ffc95ee7-885e-4d41-8276-e6680dc30890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7acfd494-d46c-4682-bd09-81f305b128b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0506f123-bad1-4445-adeb-ddb2bec0c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843, 30961]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843, 30961, 42348]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843, 30961, 42348, 15635]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843, 30961, 42348, 15635,\n",
      "         24110]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843, 30961, 42348, 15635,\n",
      "         24110, 43231]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843, 30961, 42348, 15635,\n",
      "         24110, 43231, 30967]])\n",
      "Output text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(start_context, tokenizer),\n",
    "max_new_tokens=10,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e51597e3-2243-402b-9a4e-d2b9e742913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\",\n",
    "                        [40, 1107, 588]])   # \"I really like\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "51f4f510-63a1-47ec-b045-898c5fabef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345 ],   # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])   # \" really like chocolate\"]\n",
    "                                    \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c80c9814-c54b-4040-b26e-6c8b64312ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "print(probas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "462178d1-917a-4d9e-85cb-eb5bfa49f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8804536a-b617-47f0-a2c0-f4f501c9fb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas[0][1][39619] #non zero value predicted token probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d33b2c5a-accd-4654-9d14-ae25f1494e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.0000)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#expected or targeted token probas\n",
    "probas[0][1][6100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f8a57755-909f-4f1c-82b7-d00ddf1097ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1284b657-98b2-4d01-aab6-c4b8a23b7078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 2:  really like chocolate\n",
      "Outputs batch 2:  cos slicing Aux\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 2: {token_ids_to_text(targets[1], tokenizer)}\")\n",
    "print(f\"Outputs batch 2:\"\n",
    "f\" {token_ids_to_text(token_ids[1].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8ca0d502-4841-4395-a09d-48947d34dbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36397],\n",
       "        [39619],\n",
       "        [20610]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec1c97fe-4228-47c4-98fb-1a782a53a2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36397, 39619, 20610])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cc53517b-f300-46dc-ba78-2b273eb631dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36397, 39619, 20610])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "93d639e7-b7e6-4de6-ac14-d94602fdb40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0000,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# torch.set_printoptions(sci_mode=True)\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dbcc3a3c-c1cf-4b19-aef3-6856cca7b831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "563aee42-eb0a-48a9-a1e8-6079bd6f9b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((target_probas_1, target_probas_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ae581f0c-7fbe-43b2-a814-142639b3228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "38f71c3f-ae4c-46f9-9330-914279e40708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8765)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3d6bad0c-f123-459b-877a-eebbda7ba3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "384ce06e-b35f-4b53-9836-1c02be47a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "16832349-84f4-4b84-9062-fb6d37f61ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "810b4e45-c282-47ed-8d6f-424f5f85a608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b78b79f6-0dd1-4f86-aae5-71644fa51f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 50257])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.flatten(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e0463610-754f-4c31-88b0-ee669d7e7306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3315)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat[0][3626]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b18960ea-e564-4e84-9acf-120069919b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3626,  6100,   345,  1107,   588, 11311])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "75e5ded0-6258-4b9b-8d66-c3e2065666c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a4af137d-f5d9-4741-954c-ee5d40c98aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.tensor([\n",
    "    [[0.1, 0.2, 0.7], [0.3, 0.5, 0.2], [0.4, 0.1, 0.5]],  # batch 0\n",
    "    [[0.8, 0.1, 0.1], [0.2, 0.6, 0.2], [0.5, 0.2, 0.3]]   # batch 1\n",
    "])  # Shape: (2, 3, 3) -> (batch_size=2, num_classes=3, seq_len=3)\n",
    "\n",
    "targets = torch.tensor([\n",
    "    [2, 1, 0],  # batch 0 (true labels at each step)\n",
    "    [1, 2, 0]   # batch 1\n",
    "])\n",
    "\n",
    "text_idx = [0, 1]  # Select both batch samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "38077e66-53a6-4255-b3b1-aaf1a48da2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7000, 0.5000, 0.4000])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas[0, [0, 1,2], targets[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d7709942-0d75-443c-88e2-46efc7e6d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "069449cb-4e09-4ebc-954f-ce3bc6b2bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5535faf6-be8d-4d2a-9661-781b0089cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5b2ef5ec-9cf9-42fc-b79a-7b36bd84c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "                train_data,\n",
    "                batch_size=2,\n",
    "                max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                drop_last=True,\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "                val_data,\n",
    "                batch_size=2,\n",
    "                max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                drop_last=False,\n",
    "                shuffle=False,\n",
    "                num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3c6d89f4-0657-4e20-9a02-b4cb39d29442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "\n",
      "Validation loader:\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2a899-0e0c-4c8e-87aa-409e8a9ac514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
